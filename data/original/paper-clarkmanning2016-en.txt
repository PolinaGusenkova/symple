Coreference resolution systems typically operate by making sequences of local decisions (e.g., adding a coreference link between two mentions).
However, most measures of coreference resolution performance do not decompose over local decisions, which means the utility of a particular decision is not known until all other decisions have been made.
Due to this difficulty, coreference systems are usually trained with loss functions that heuristically define the goodness of a particular coreference decision.
These losses contain hyperparameters that are carefully selected to ensure the model performs well according to coreference evaluation metrics.
This complicates training, especially across different languages and datasets where systems may work best with different settings of the hyperparameters.
To address this, we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics.
In particular, we modify the max-margin coreference objective proposed by Wiseman et al. (2015) by incorporating the reward associated with each coreference decision into the loss’s slack rescaling.
We also test the REINFORCE policy gradient algorithm.
Our model is a neural mention-ranking model.
Mention-ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters.
Hence they operate in a simple setting where coreference decisions are made independently.
Although they are less expressive than entity-centric approaches to coreference, mention-ranking models are fast, scalable, and simple to train, causing them to be the dominant approach to coreference in recent years. 
Having independent actions is particularly useful when applying reinforcement learning because it means a particular action’s effect on the final reward can be computed efficiently.
We evaluate the models on the English and Chinese portions of the CoNLL 2012 Shared Task.
The REINFORCE algorithm is competitive with a heuristic loss function while the reward-rescaled objective significantly outperforms both.
We attribute this to reward rescaling being well suited for a ranking task due to its max-margin loss as well as benefiting from directly optimizing for coreference metrics.
Error analysis shows that using the reward-rescaling loss results in a similar number of mistakes as the heuristic loss, but the mistakes tend to be less severe.
We use the neural mention-ranking model described in Clark and Manning (2016), which we briefly go over in this section.
Given a mention m and candidate antecedent c, the mention-ranking model produces a score for the pair s(c, m) indicating their compatibility for coreference with a feedforward neural network.
The candidate antecedent may be any mention that occurs before m in the document or NA, indicating that m has no antecedent.
For each mention, the model extracts various words (e.g., the mention’s head word) and groups of words (e.g., all words in the mention’s sentence) that are fed into the neural network.
Each word is represented by a vector wi ∈ Rdw.
Each group of words is represented by the average of the vectors of each word in the group.
In addition to the embeddings, a small number of additional features are used, including distance, string matching, and speaker identification features.
See Clark and Manning (2016) for the full set of features and an ablation study.